{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc08440-aafb-4856-9ea6-f0acdc95ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is the connection to BigQuery and collection to events_ table (daily not interdate) to get events from the last 7 days in order to create the \n",
    "# source of truth that can be used as the reference for further comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20524e58-5912-4b57-a85c-684ac4aa7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86870c-b3af-4e6c-b880-34459906fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "print(\"Kernel Python:\", sys.executable)\n",
    "# 1) Avoid the default cache (moves it to a safe ASCII path)\n",
    "os.environ[\"PIP_CACHE_DIR\"] = r\"C:\\pipcache\"\n",
    "\n",
    "# 2) Upgrade pip tooling in THIS kernel\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# 3) Install your packages without using the cache (prevents file lock issues)\n",
    "%pip install --no-cache-dir google-cloud-bigquery PyYAML requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ac999-da25-4a25-a981-f03ec84363e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-cloud-bigquery PyYAML requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b56217-03d4-4aa2-a83b-1f062ce7cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfa227-736f-4e0a-8f51-360046ff6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas pyarrow db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e2fa9-b691-4600-9483-b27c8835eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (only once per environment)\n",
    "# !pip install google-auth google-auth-oauthlib google-cloud-bigquery\n",
    "\n",
    "# BigQuery connection\n",
    "# 1️⃣ Your GCP “App” project\n",
    "# This is the one you used to create the client_secret.json file. It’s the project where you:\n",
    "# - Created the OAuth consent screen\n",
    "# - Created the OAuth Client ID (Desktop App)\n",
    "# - Possibly store your own scripts or Cloud Functions\n",
    "# This project is used only for authentication and API access — it doesn’t need to be the same one that contains the GA4 data. It provides the credentials you use in your notebook.\n",
    "\n",
    "# 2️⃣ The GA4 BigQuery export project\n",
    "# This is a separate project, automatically created or linked by Google Analytics when you set up the BigQuery export in GA4 Admin → BigQuery Linking.\n",
    "# This project contains:\n",
    "# - The GA4 dataset (usually named analytics_<propertyid>)\n",
    "# - Tables like events_20251110, events_intraday_20251111, etc.\n",
    "# This project is where your data lives, and it’s the one you need read access to.\n",
    "\n",
    "import os, json\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# ----- EDIT THESE -----\n",
    "CLIENT_SECRET_FILE = r\"YOURPATH\\client_secret.json\"\n",
    "PROJECT_ID = \"YOUR-PROJECT-ID\"\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/bigquery\",\n",
    "    \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "]\n",
    "TOKEN_PATH = \"token.json\"\n",
    "# ----------------------\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/bigquery\"]\n",
    "flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\n",
    "creds = flow.run_local_server(port=0)\n",
    "\n",
    "# Note: project here is the GA4 one, not the OAuth one\n",
    "client = bigquery.Client(project=\"YOUR-PROJECT-ID\", credentials=creds)\n",
    "\n",
    "for d in client.list_datasets():\n",
    "    print(\" -\", d.dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d1513-38ab-4cef-964e-a4114b699088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import re, datetime as dt\n",
    "\n",
    "# --- set these two correctly ---\n",
    "BQ_PROJECT = \"YOUR-PROJECT-ID\"           # GA4 export project \n",
    "BQ_DATASET = \"YOUR-DATASET\"        # GA4 dataset name from GA4 Admin → BigQuery Linking \n",
    "BQ_TABLE_WC  = \"events_*\"                  # GA4 export table wildcard\n",
    "BQ_LOCATION  = \"EU\"                        # \"EU\" or \"US\" (match your dataset)\n",
    "# --------------------------------\n",
    "\n",
    "# 1) Discover dataset location (EU/US) - got some issue if sending the location straight\n",
    "meta_client = bigquery.Client(project=BQ_PROJECT, credentials=creds)  # no location here\n",
    "ds = meta_client.get_dataset(f\"{BQ_PROJECT}.{BQ_DATASET}\")            # raises NotFound if typo\n",
    "DATASET_LOCATION = ds.location\n",
    "print(\"Dataset location:\", DATASET_LOCATION)\n",
    "\n",
    "# 2) Create a client bound to the correct location\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=creds, location=DATASET_LOCATION)\n",
    "\n",
    "# 3) Find the newest events table (daily first, else intraday)\n",
    "tables = list(client.list_tables(f\"{BQ_PROJECT}.{BQ_DATASET}\"))\n",
    "latest_daily, latest_intra = None, None\n",
    "for t in tables:\n",
    "    if re.match(r\"events_\\d{8}$\", t.table_id):\n",
    "        latest_daily = max(latest_daily or t.table_id, t.table_id)\n",
    "    elif re.match(r\"events_intraday_\\d{8}$\", t.table_id):\n",
    "        latest_intra = max(latest_intra or t.table_id, t.table_id)\n",
    "\n",
    "latest = latest_daily or latest_intra\n",
    "print(\"Latest table:\", latest)\n",
    "assert latest, \"No GA4 events tables found. Check link/export or permissions.\"\n",
    "\n",
    "# 4) Quick check: Preview a few rows from that specific table\n",
    "sql = f\"\"\"\n",
    "SELECT event_name, COUNT(*) AS c\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{latest}`\n",
    "GROUP BY event_name\n",
    "ORDER BY c DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "# run your query\n",
    "job = client.query(sql)  # client already created with correct location\n",
    "rows = list(job.result())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9189fd1-327f-42ec-881b-f23021e76fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# assumes you already have: creds, BQ_PROJECT, BQ_DATASET, DATASET_LOCATION (europe-north1)\n",
    "meta_client = bigquery.Client(project=BQ_PROJECT, credentials=creds)\n",
    "ds = meta_client.get_dataset(f\"{BQ_PROJECT}.{BQ_DATASET}\")\n",
    "DATASET_LOCATION = ds.location  # keep using europe-north1\n",
    "\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=creds, location=DATASET_LOCATION)\n",
    "\n",
    "def newest_tables(client, project, dataset, max_tables=3):\n",
    "    tables = list(client.list_tables(f\"{project}.{dataset}\"))\n",
    "    parsed = []\n",
    "    for t in tables:\n",
    "        tid = t.table_id\n",
    "        m = re.match(r\"(events|events_intraday)_(\\d{8})$\", tid)\n",
    "        if m:\n",
    "            parsed.append((tid, datetime.strptime(m.group(2), \"%Y%m%d\"), m.group(1)))\n",
    "    # sort by date desc and pick a few\n",
    "    parsed.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [tid for (tid, _, _) in parsed[:max_tables]]\n",
    "\n",
    "latest_tbls = newest_tables(client, BQ_PROJECT, BQ_DATASET, max_tables=3)\n",
    "print(\"Using tables:\", latest_tbls)\n",
    "assert latest_tbls, \"No GA4 events tables found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc930d-625d-4bba-938a-977be7509088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, math, re\n",
    "# PII CHECK FOR PARAMETERS\n",
    "# ✅ raw strings for regex to avoid SyntaxWarning\n",
    "PII_KEY_RE = re.compile(r\"(email|name|phone|address|login|ssn|iban)\", re.I)\n",
    "PII_VAL_RE = re.compile(r\"@|(\\+\\d{6,})|[0-9]{8,}\")\n",
    "\n",
    "def generate_autospec_fast(client, project, dataset, tables, sample_rate=0.002, out_path=\"tracking_spec.generated.yml\"):\n",
    "    # Build a UNION over latest tables\n",
    "    union_sql = \" \\nUNION ALL\\n\".join([f\"SELECT event_name, event_params, items FROM `{project}.{dataset}.{t}`\" for t in tables])\n",
    "\n",
    "    # ✅ triple quotes properly closed, BOOL_OR replaced with COUNTIF(...) > 0\n",
    "    sql = f\"\"\"\n",
    "    WITH unioned AS (\n",
    "      {union_sql}\n",
    "    ),\n",
    "    sampled AS (\n",
    "      SELECT * FROM unioned\n",
    "      WHERE RAND() < {sample_rate}\n",
    "      LIMIT 200000\n",
    "    ),\n",
    "    flat AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        ep.key AS param_key,\n",
    "        COALESCE(\n",
    "          ep.value.string_value,\n",
    "          CAST(ep.value.int_value AS STRING),\n",
    "          CAST(ep.value.double_value AS STRING)\n",
    "        ) AS param_val,\n",
    "        (items IS NOT NULL AND ARRAY_LENGTH(items) > 0) AS items_present,\n",
    "        SAFE_CAST(COALESCE(\n",
    "          ep.value.string_value,\n",
    "          CAST(ep.value.int_value AS STRING),\n",
    "          CAST(ep.value.double_value AS STRING)\n",
    "        ) AS FLOAT64) AS num_val\n",
    "      FROM sampled, UNNEST(event_params) ep\n",
    "    ),\n",
    "    stats AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        param_key,\n",
    "        COUNT(*) AS rows_total,\n",
    "        COUNTIF(param_val IS NOT NULL AND param_val != '') AS rows_present,\n",
    "        COUNTIF(num_val IS NOT NULL) AS num_count,\n",
    "        MIN(num_val) AS min_num,\n",
    "        ARRAY_AGG(DISTINCT param_val IGNORE NULLS LIMIT 50) AS samples,\n",
    "        COUNTIF(items_present) > 0 AS items_present_any\n",
    "      FROM flat\n",
    "      GROUP BY 1,2\n",
    "    )\n",
    "    SELECT * FROM stats\n",
    "    ORDER BY event_name, param_key\n",
    "    \"\"\"\n",
    "    rows = client.query(sql).result()\n",
    "    summary = [dict(r) for r in rows]\n",
    "\n",
    "    # Build YAML\n",
    "    out = {\"meta\": {\"spec_version\": \"auto-1.0\"}, \"events\": []}\n",
    "    req_threshold = 0.98\n",
    "\n",
    "    from collections import defaultdict\n",
    "    by_event = defaultdict(list)\n",
    "    items_presence = defaultdict(bool)\n",
    "\n",
    "    for r in summary:\n",
    "        ev = r[\"event_name\"]\n",
    "        k = r[\"param_key\"]\n",
    "        rows_total = r[\"rows_total\"] or 1\n",
    "        rows_present = r[\"rows_present\"] or 0\n",
    "        presence_rate = rows_present / rows_total\n",
    "        num_count = r[\"num_count\"] or 0\n",
    "        min_num = r[\"min_num\"]\n",
    "        samples = r[\"samples\"] or []\n",
    "        items_presence[ev] = items_presence[ev] or bool(r[\"items_present_any\"])\n",
    "\n",
    "        # skip obvious PII\n",
    "        if PII_KEY_RE.search(k or \"\") or any(PII_VAL_RE.search((s or \"\")) for s in samples[:20]):\n",
    "            continue\n",
    "\n",
    "        # type inference\n",
    "        if num_count >= max(5, math.ceil(0.5 * len(samples))):\n",
    "            ptype = \"integer\" if (min_num is not None and float(min_num).is_integer()) else \"number\"\n",
    "        else:\n",
    "            ptype = \"string\"\n",
    "\n",
    "        enum = None\n",
    "        if ptype == \"string\" and 0 < len(samples) <= 20 and presence_rate >= 0.95:\n",
    "            enum = sorted(samples)\n",
    "\n",
    "        entry = {\"key\": k, \"type\": ptype, \"required\": presence_rate >= req_threshold}\n",
    "        if ptype in (\"number\", \"integer\") and min_num is not None and min_num >= 0:\n",
    "            entry[\"min\"] = 0\n",
    "        if enum:\n",
    "            entry[\"enum\"] = enum\n",
    "\n",
    "        by_event[ev].append(entry)\n",
    "\n",
    "    for ev, params in by_event.items():\n",
    "        ev_entry = {\"name\": ev, \"required_params\": params}\n",
    "        if items_presence[ev]:\n",
    "            ev_entry[\"required_params\"].append({\n",
    "                \"key\": \"items\", \"type\": \"array<object>\", \"required\": True,\n",
    "                \"item_schema\": [\n",
    "                    {\"key\": \"item_id\", \"type\": \"string\", \"required\": True},\n",
    "                    {\"key\": \"quantity\", \"type\": \"integer\", \"required\": True, \"min\": 1},\n",
    "                    {\"key\": \"price\", \"type\": \"number\", \"required\": False, \"min\": 0}\n",
    "                ]\n",
    "            })\n",
    "        out[\"events\"].append(ev_entry)\n",
    "\n",
    "    # ✅ raw strings used in regex-like text patterns\n",
    "    out[\"consent\"] = {\"required_storages\": [\"analytics_storage\", \"ad_storage\"], \"marketing_vendors_block_when_denied\": True}\n",
    "    out[\"pii_policy\"] = {\n",
    "        \"disallow_param_keys\": [\"email\", \"phone\", \"first_name\", \"last_name\", \"address\"],\n",
    "        \"text_patterns\": [r\"(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\", r\"(?i)\\b(?:\\+?\\d{1,3}[\\s-]?)?(?:\\d[\\s-]?){8,}\\b\"]\n",
    "    }\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(out, f, sort_keys=False, allow_unicode=True)\n",
    "    return out_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1564d7-4033-4b9d-990d-997229ae235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect-only URL-param PII scanner\n",
    "import yaml, math, re\n",
    "\n",
    "# ----------------------------\n",
    "# PII CHECK FOR PARAMETERS\n",
    "# ----------------------------\n",
    "EMAIL_RE = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\",re.IGNORECASE,)\n",
    "PHONE_RE = re.compile(\n",
    "    r\"\"\"\n",
    "    \\b\n",
    "    (?:\\+|00)\\d            # +358 / 0044 etc.\n",
    "    [\\d\\-\\s]{5,16}         # allow spaces/dashes but total length reasonable\n",
    "    \\b\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "LONG_NUMERIC_RE = re.compile(r\"\\b\\d{9,}\\b\")\n",
    "IPV4_RE = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "IPV6_RE = re.compile(r\"\\b(?:[0-9a-f]{1,4}:){2,7}[0-9a-f]{1,4}\\b\", re.IGNORECASE,)\n",
    "CREDITCARD_RE = re.compile(r\"\\b(?:\\d[ -]*?){13,19}\\b\")\n",
    "FINNISH_HETU_RE = re.compile(r\"\\b\\d{6}[-+A]\\d{3}[0-9A-FHJ-NPR-Y]\\b\", re.IGNORECASE,)\n",
    "PII_VALUE_PATTERNS = [\n",
    "    EMAIL_RE,\n",
    "    PHONE_RE,\n",
    "    IPV4_RE,\n",
    "    IPV6_RE,\n",
    "    CREDITCARD_RE,\n",
    "    FINNISH_HETU_RE,\n",
    "    LONG_NUMERIC_RE,\n",
    "]\n",
    "PII_VALUE_PATTERNS_LOOSE = [EMAIL_RE, PHONE_RE, IPV4_RE, IPV6_RE]\n",
    "PII_VALUE_PATTERNS_STRICT = PII_VALUE_PATTERNS_LOOSE + [CREDITCARD_RE, FINNISH_HETU_RE, LONG_NUMERIC_RE]\n",
    "\n",
    "def looks_like_pii_value(value: str) -> bool:\n",
    "    if not value:\n",
    "        return False\n",
    "    v = str(value)\n",
    "    return any(p.search(v) for p in PII_VALUE_PATTERNS_STRICT)\n",
    "\n",
    "\n",
    "PII_KEY_RE = re.compile(r\"(email|name|phone|address|login|ssn|iban)\", re.I)\n",
    "PII_VAL_RE = re.compile(r\"@|(\\+\\d{6,})|[0-9]{8,}\")\n",
    "\n",
    "# Which GA4 params commonly carry full URLs\n",
    "URL_PARAM_KEYS = (\"page_location\", \"page_referrer\", \"link_url\")\n",
    "\n",
    "\n",
    "def generate_autospec_fast(\n",
    "    client,\n",
    "    project,\n",
    "    dataset,\n",
    "    tables,\n",
    "    sample_rate=0.002,\n",
    "    out_path=\"tracking_spec.generated.yml\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a sampled inventory of GA4 event parameters, infers simple types, skips obvious PII,\n",
    "    and (NEW) detects PII in URL query parameters (detect-only). Outputs a YAML spec plus\n",
    "    a 'pii_url_findings' section for easy review / alerting.\n",
    "\n",
    "    Args:\n",
    "        client: google.cloud.bigquery.Client\n",
    "        project: GCP project id\n",
    "        dataset: BigQuery dataset id for GA4 export\n",
    "        tables: list[str] of table ids to UNION (e.g., last N days)\n",
    "        sample_rate: float (0..1), probabilistic sampling per row\n",
    "        out_path: path to write YAML spec\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # 1) Parameter stats (original logic)\n",
    "    # ----------------------------\n",
    "    union_sql = \" \\nUNION ALL\\n\".join(\n",
    "        [f\"SELECT event_name, event_params, items FROM `{project}.{dataset}.{t}`\" for t in tables]\n",
    "    )\n",
    "\n",
    "    # NOTE: This is a normal f-string. The SQL here doesn't use backslash-heavy regex,\n",
    "    # so we don't need the raw prefix (r or rf). Where we *do* use regex in SQL later,\n",
    "    # we use an rf\"\"\"...\"\"\" string and double braces {{...}} for quantifiers.\n",
    "    sql = f\"\"\"\n",
    "    WITH unioned AS (\n",
    "      {union_sql}\n",
    "    ),\n",
    "    sampled AS (\n",
    "      SELECT * FROM unioned\n",
    "      WHERE RAND() < {sample_rate}\n",
    "      LIMIT 200000\n",
    "    ),\n",
    "    flat AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        ep.key AS param_key,\n",
    "        COALESCE(\n",
    "          ep.value.string_value,\n",
    "          CAST(ep.value.int_value AS STRING),\n",
    "          CAST(ep.value.double_value AS STRING)\n",
    "        ) AS param_val,\n",
    "        (items IS NOT NULL AND ARRAY_LENGTH(items) > 0) AS items_present,\n",
    "        SAFE_CAST(COALESCE(\n",
    "          ep.value.string_value,\n",
    "          CAST(ep.value.int_value AS STRING),\n",
    "          CAST(ep.value.double_value AS STRING)\n",
    "        ) AS FLOAT64) AS num_val\n",
    "      FROM sampled, UNNEST(event_params) ep\n",
    "    ),\n",
    "    stats AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        param_key,\n",
    "        COUNT(*) AS rows_total,\n",
    "        COUNTIF(param_val IS NOT NULL AND param_val != '') AS rows_present,\n",
    "        COUNTIF(num_val IS NOT NULL) AS num_count,\n",
    "        MIN(num_val) AS min_num,\n",
    "        ARRAY_AGG(DISTINCT param_val IGNORE NULLS LIMIT 50) AS samples,\n",
    "        COUNTIF(items_present) > 0 AS items_present_any\n",
    "      FROM flat\n",
    "      GROUP BY 1,2\n",
    "    )\n",
    "    SELECT * FROM stats\n",
    "    ORDER BY event_name, param_key\n",
    "    \"\"\"\n",
    "    rows = client.query(sql).result()\n",
    "    summary = [dict(r) for r in rows]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) URL-parameter PII detector (detect-only)\n",
    "    # ----------------------------\n",
    "    # Use a *raw f-string* so backslashes in regex aren't treated as Python escapes.\n",
    "    # IMPORTANT: In f-strings, '{' and '}' are special, so we double them for regex quantifiers: {{2,}}, {{7,}}, etc.\n",
    "    url_pii_sql = rf\"\"\"\n",
    "    WITH unioned AS (\n",
    "      {union_sql}\n",
    "    ),\n",
    "    sampled AS (\n",
    "      SELECT * FROM unioned\n",
    "      WHERE RAND() < {sample_rate}\n",
    "      LIMIT 200000\n",
    "    ),\n",
    "    urls AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        ep.key AS param_key,\n",
    "        ep.value.string_value AS url_val\n",
    "      FROM sampled, UNNEST(event_params) ep\n",
    "      WHERE ep.key IN ('page_location','page_referrer','link_url')\n",
    "        AND ep.value.string_value IS NOT NULL\n",
    "        AND ep.value.string_value LIKE '%?%'\n",
    "    ),\n",
    "    pairs AS (\n",
    "      SELECT\n",
    "        event_name, param_key, url_val,\n",
    "        SPLIT(url_val, '?')[SAFE_OFFSET(1)] AS query_str\n",
    "      FROM urls\n",
    "    ),\n",
    "    kv AS (\n",
    "      SELECT\n",
    "        event_name, param_key, url_val,\n",
    "        kv AS pair,\n",
    "        LOWER(SPLIT(kv, '=')[SAFE_OFFSET(0)]) AS k,\n",
    "        SPLIT(kv, '=')[SAFE_OFFSET(1)] AS v_raw\n",
    "      FROM pairs, UNNEST(SPLIT(query_str, '&')) AS kv\n",
    "    ),\n",
    "    norm AS (\n",
    "      SELECT\n",
    "        event_name, param_key, url_val, k,\n",
    "        SAFE.SUBSTR(TRIM(v_raw), 0, 2048) AS v,\n",
    "        REPLACE(TRIM(v_raw), '%40', '@') AS v_at\n",
    "      FROM kv\n",
    "      WHERE v_raw IS NOT NULL\n",
    "    ),\n",
    "    matches AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        param_key,\n",
    "        k AS query_param,\n",
    "        CASE\n",
    "          WHEN REGEXP_CONTAINS(LOWER(v_at),\n",
    "               r'(^|[^a-z0-9._%+\\-])[a-z0-9._%+\\-]+@[^ \\s/?#&]+\\.[a-z]{{2,}}([^a-z]|$)')\n",
    "            THEN 'email'\n",
    "          WHEN REGEXP_CONTAINS(v, r'(^|\\D)(\\+?\\d[\\d\\-\\s() ]{{7,}})(\\D|$)')\n",
    "            THEN 'phone'\n",
    "          WHEN REGEXP_CONTAINS(LOWER(k), r'(email|e-?mail|user(name)?|login|phone|mobile|msisdn|ssn|nid|token|auth|session|address)')\n",
    "            THEN 'suspicious_key'\n",
    "          WHEN REGEXP_CONTAINS(v, r'([A-Fa-f0-9]{{16,}}|[A-Za-z0-9_\\-]{{24,}})')\n",
    "            THEN 'id_like'\n",
    "          ELSE NULL\n",
    "        END AS pii_type,\n",
    "        -- Mask local parts of emails and long numbers in the *full* URL for safe sharing\n",
    "        REGEXP_REPLACE(\n",
    "          REGEXP_REPLACE(url_val, r'([a-zA-Z0-9._%+\\-]+)@', r'***@'),\n",
    "          r'(\\d{{6,}})', '***'\n",
    "        ) AS sample_url_masked\n",
    "      FROM norm\n",
    "      WHERE\n",
    "        REGEXP_CONTAINS(LOWER(v_at),\n",
    "          r'(^|[^a-z0-9._%+\\-])[a-z0-9._%+\\-]+(@|%40)[^ \\s/?#&]+\\.[a-z]{{2,}}([^a-z]|$)')\n",
    "        OR REGEXP_CONTAINS(v, r'(^|\\D)(\\+?\\d[\\d\\-\\s() ]{{7,}})(\\D|$)')\n",
    "        OR REGEXP_CONTAINS(LOWER(k), r'(email|e-?mail|user(name)?|login|phone|mobile|msisdn|ssn|nid|token|auth|session|address)')\n",
    "        OR REGEXP_CONTAINS(v, r'([A-Fa-f0-9]{{16,}}|[A-Za-z0-9_\\-]{{24,}})')\n",
    "    ),\n",
    "    agg AS (\n",
    "      SELECT\n",
    "        event_name,\n",
    "        param_key,\n",
    "        query_param,\n",
    "        pii_type,\n",
    "        COUNT(*) AS events_flagged,\n",
    "        ARRAY_AGG(DISTINCT sample_url_masked IGNORE NULLS LIMIT 10) AS sample_urls_masked\n",
    "      FROM matches\n",
    "      WHERE pii_type IS NOT NULL\n",
    "      GROUP BY 1,2,3,4\n",
    "    )\n",
    "    SELECT * FROM agg\n",
    "    ORDER BY events_flagged DESC\n",
    "    \"\"\"\n",
    "    url_rows = client.query(url_pii_sql).result()\n",
    "    url_findings = [dict(r) for r in url_rows]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Build YAML (original logic + new pii_url_findings)\n",
    "    # ----------------------------\n",
    "    out = {\"meta\": {\"spec_version\": \"auto-1.0\"}, \"events\": []}\n",
    "    req_threshold = 0.98\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    by_event = defaultdict(list)\n",
    "    items_presence = defaultdict(bool)\n",
    "\n",
    "    for r in summary:\n",
    "        ev = r[\"event_name\"]\n",
    "        k = r[\"param_key\"]\n",
    "        rows_total = r[\"rows_total\"] or 1\n",
    "        rows_present = r[\"rows_present\"] or 0\n",
    "        presence_rate = rows_present / rows_total\n",
    "        num_count = r[\"num_count\"] or 0\n",
    "        min_num = r[\"min_num\"]\n",
    "        samples = r[\"samples\"] or []\n",
    "        items_presence[ev] = items_presence[ev] or bool(r[\"items_present_any\"])\n",
    "\n",
    "        # Skip obvious PII (existing behavior)\n",
    "        if PII_KEY_RE.search(k or \"\") or any(\n",
    "            looks_like_pii_value(s) for s in samples[:20]\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Type inference\n",
    "        if num_count >= max(5, math.ceil(0.5 * len(samples))):\n",
    "            ptype = \"integer\" if (min_num is not None and float(min_num).is_integer()) else \"number\"\n",
    "        else:\n",
    "            ptype = \"string\"\n",
    "\n",
    "        enum = None\n",
    "        if ptype == \"string\" and 0 < len(samples) <= 20 and presence_rate >= 0.95:\n",
    "            enum = sorted(samples)\n",
    "\n",
    "        entry = {\"key\": k, \"type\": ptype, \"required\": presence_rate >= req_threshold}\n",
    "        if ptype in (\"number\", \"integer\") and min_num is not None and min_num >= 0:\n",
    "            entry[\"min\"] = 0\n",
    "        if enum:\n",
    "            entry[\"enum\"] = enum\n",
    "\n",
    "        by_event[ev].append(entry)\n",
    "\n",
    "    for ev, params in by_event.items():\n",
    "        ev_entry = {\"name\": ev, \"required_params\": params}\n",
    "        if items_presence[ev]:\n",
    "            ev_entry[\"required_params\"].append(\n",
    "                {\n",
    "                    \"key\": \"items\",\n",
    "                    \"type\": \"array<object>\",\n",
    "                    \"required\": True,\n",
    "                    \"item_schema\": [\n",
    "                        {\"key\": \"item_id\", \"type\": \"string\", \"required\": True},\n",
    "                        {\"key\": \"quantity\", \"type\": \"integer\", \"required\": True, \"min\": 1},\n",
    "                        {\"key\": \"price\", \"type\": \"number\", \"required\": False, \"min\": 0},\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "        out[\"events\"].append(ev_entry)\n",
    "\n",
    "    # Policy (unchanged)\n",
    "    out[\"consent\"] = {\n",
    "        \"required_storages\": [\"analytics_storage\", \"ad_storage\"],\n",
    "        \"marketing_vendors_block_when_denied\": True,\n",
    "    }\n",
    "    out[\"pii_policy\"] = {\n",
    "        \"disallow_param_keys\": [\"email\", \"phone\", \"first_name\", \"last_name\", \"address\"],\n",
    "        \"text_patterns\": [\n",
    "            r\"(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\",\n",
    "            r\"(?i)\\b(?:\\+?\\d{1,3}[\\s-]?)?(?:\\d[\\s-]?){8,}\\b\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # NEW: attach detect-only URL PII summary for BI/Slack\n",
    "    per_event = defaultdict(list)\n",
    "    for r in url_findings:\n",
    "        per_event[r[\"event_name\"]].append(\n",
    "            {\n",
    "                \"page_param\": r[\"param_key\"],\n",
    "                \"query_param\": r[\"query_param\"],\n",
    "                \"pii_type\": r[\"pii_type\"],\n",
    "                \"events_flagged\": int(r[\"events_flagged\"]),\n",
    "                \"sample_urls_masked\": r.get(\"sample_urls_masked\", []),\n",
    "            }\n",
    "        )\n",
    "    out[\"pii_url_findings\"] = [{\"event\": ev, \"offenders\": offenders} for ev, offenders in per_event.items()]\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(out, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433be09-9970-4f53-9da9-1831d932936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = generate_autospec_fast(client, BQ_PROJECT, BQ_DATASET, latest_tbls, sample_rate=0.002)\n",
    "print(\"✅ Wrote:\", yaml_path)\n",
    "\n",
    "# quick peek\n",
    "with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 60: break\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa3e97-4dc6-4519-b835-c06502620a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sampled, fast PII audit over latest GA4 tables ---\n",
    "from google.cloud.bigquery import QueryJobConfig\n",
    "\n",
    "SAMPLE_RATE = 0.001   # 0.1% sample; raise/lower if needed\n",
    "MAX_ROWS    = 200000  # safety cap for sampled CTE\n",
    "BYTES_CAP   = 2 * 1024**3  # 2 GB guardrail; adjust if needed\n",
    "\n",
    "union_sql = \" \\nUNION ALL\\n\".join([\n",
    "    f\"SELECT event_name, event_params, user_properties, items FROM `{BQ_PROJECT}.{BQ_DATASET}.{t}`\"\n",
    "    for t in latest_tbls\n",
    "])\n",
    "\n",
    "sql_pii = f\"\"\"\n",
    "WITH base AS (\n",
    "  {union_sql}\n",
    "),\n",
    "sampled AS (\n",
    "  SELECT * FROM base\n",
    "  WHERE RAND() < {SAMPLE_RATE}\n",
    "  LIMIT {MAX_ROWS}\n",
    "),\n",
    "-- 1) event_params\n",
    "ev AS (\n",
    "  SELECT\n",
    "    'event_param' AS source,\n",
    "    event_name,\n",
    "    ep.key AS param_key,\n",
    "    COALESCE(ep.value.string_value,\n",
    "             CAST(ep.value.int_value AS STRING),\n",
    "             CAST(ep.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(event_params) ep\n",
    "),\n",
    "-- 2) user_properties (GA4 structure is similar to event_params)\n",
    "up AS (\n",
    "  SELECT\n",
    "    'user_property' AS source,\n",
    "    event_name,\n",
    "    up.key AS param_key,\n",
    "    COALESCE(up.value.string_value,\n",
    "             CAST(up.value.int_value AS STRING),\n",
    "             CAST(up.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(user_properties) up\n",
    "),\n",
    "-- 3) item parameters (if any)\n",
    "ip AS (\n",
    "  SELECT\n",
    "    'item_param' AS source,\n",
    "    event_name,\n",
    "    ip.key AS param_key,\n",
    "    COALESCE(ip.value.string_value,\n",
    "             CAST(ip.value.int_value AS STRING),\n",
    "             CAST(ip.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(items) it, UNNEST(it.item_params) ip\n",
    "),\n",
    "flat AS (\n",
    "  SELECT * FROM ev\n",
    "  UNION ALL SELECT * FROM up\n",
    "  UNION ALL SELECT * FROM ip\n",
    ")\n",
    "SELECT\n",
    "  source,\n",
    "  event_name,\n",
    "  param_key,\n",
    "  COUNT(*) AS hits,\n",
    "  ARRAY_AGG(DISTINCT SUBSTR(param_value,1,80) IGNORE NULLS LIMIT 5) AS sample_values\n",
    "FROM flat\n",
    "WHERE\n",
    "  -- suspicious keys\n",
    "  REGEXP_CONTAINS(LOWER(param_key), r'(email|e-mail|mail|name|firstname|lastname|phone|mobile|tel|address|street|zip|postcode|login|user|ssn|iban)')\n",
    "  -- OR suspicious values: email-like or long digit/phone patterns\n",
    "  OR REGEXP_CONTAINS(LOWER(param_value), r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{{2,}}')\n",
    "  OR REGEXP_CONTAINS(param_value, r'(?i)\\\\b(?:\\\\+?\\\\d{{1,3}}[\\\\s-]?)?(?:\\\\d[\\\\s-]? ){{8,}}\\\\b')\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY hits DESC, source, event_name, param_key\n",
    "\"\"\"\n",
    "\n",
    "job_cfg = QueryJobConfig(maximum_bytes_billed=BYTES_CAP)\n",
    "pii_rows = list(client.query(sql_pii, job_config=job_cfg).result())\n",
    "print(f\"Rows flagged: {len(pii_rows)}\")\n",
    "\n",
    "# Show a quick preview\n",
    "for r in pii_rows[:15]:\n",
    "    print(f\"[{r['source']}] {r['event_name']} :: {r['param_key']}  hits={r['hits']}  samples={r['sample_values']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ba32f-f395-4449-9ad7-8ccf51106b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_filename = f\"{BQ_DATASET}_pii_audit_results.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"source\",\"event_name\",\"param_key\",\"hits\",\"sample_values\"])\n",
    "    for r in pii_rows:\n",
    "        w.writerow([r[\"source\"], r[\"event_name\"], r[\"param_key\"], r[\"hits\"], \"; \".join(r[\"sample_values\"] or [])])\n",
    "\n",
    "print(\"Wrote: pii_audit_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e76f95-27fc-48ae-93fd-524384389671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.bigquery import QueryJobConfig\n",
    "\n",
    "def run_sql(sql, bytes_cap=2 * 1024**3):\n",
    "    \"\"\"Executes SQL and prints detailed BigQuery errors if they occur.\"\"\"\n",
    "    cfg = QueryJobConfig(maximum_bytes_billed=bytes_cap)\n",
    "    job = client.query(sql, job_config=cfg)\n",
    "    try:\n",
    "        rows = list(job.result())\n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        print(\"---- BigQuery error details ----\")\n",
    "        try:\n",
    "            print(job.errors)  # detailed JSON error block\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"Exception:\", e)\n",
    "        raise\n",
    "\n",
    "def dryrun_sql(sql):\n",
    "    \"\"\"Performs a dry run (syntax + cost check, no data scanned).\"\"\"\n",
    "    cfg = QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    job = client.query(sql, job_config=cfg)\n",
    "    print(f\"✅ Query syntax OK — estimated bytes processed: {job.total_bytes_processed:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c7370-b0b2-45c7-8905-a9e79db6185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 0.001\n",
    "MAX_ROWS = 150000\n",
    "\n",
    "union_sql = \" \\nUNION ALL\\n\".join([\n",
    "    f\"SELECT event_name, event_params FROM `{BQ_PROJECT}.{BQ_DATASET}.{t}`\"\n",
    "    for t in latest_tbls[:1]  # start with just the newest table\n",
    "])\n",
    "\n",
    "sql_min = f\"\"\"\n",
    "WITH base AS (\n",
    "  {union_sql}\n",
    "),\n",
    "sampled AS (\n",
    "  SELECT * FROM base\n",
    "  WHERE RAND() < {SAMPLE_RATE}\n",
    "  LIMIT {MAX_ROWS}\n",
    "),\n",
    "flat AS (\n",
    "  SELECT\n",
    "    event_name,\n",
    "    ep.key AS param_key,\n",
    "    COALESCE(ep.value.string_value,\n",
    "             CAST(ep.value.int_value AS STRING),\n",
    "             CAST(ep.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(event_params) ep\n",
    "),\n",
    "flags AS (\n",
    "  SELECT\n",
    "    event_name, param_key, param_value,\n",
    "    REGEXP_CONTAINS(LOWER(param_key),\n",
    "      r'(email|e-mail|mail|name|firstname|first_name|lastname|last_name|phone|mobile|tel|address|street|zip|postcode|iban|ssn|nationalid|hetu)'\n",
    "    ) AS key_suspicious,\n",
    "    REGEXP_CONTAINS(LOWER(param_value),\n",
    "      r'^[a-z0-9._%+-]+@[a-z0-9.-]+\\\\.[a-z]{{2,}}$'\n",
    "    ) AS looks_email\n",
    "  FROM flat\n",
    ")\n",
    "SELECT\n",
    "  'event_param' AS source,\n",
    "  event_name, param_key,\n",
    "  COUNT(*) AS hits,\n",
    "  ARRAY_AGG(DISTINCT SUBSTR(param_value,1,120) IGNORE NULLS LIMIT 5) AS sample_values\n",
    "FROM flags\n",
    "WHERE key_suspicious OR looks_email\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY hits DESC, event_name, param_key\n",
    "\"\"\"\n",
    "\n",
    "dryrun_sql(sql_min)\n",
    "rows_min = run_sql(sql_min)\n",
    "print(\"Rows flagged (minimal):\", len(rows_min))\n",
    "rows_min[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d099033-387c-498a-88f6-7f6c20bc8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_sql = \" \\nUNION ALL\\n\".join([\n",
    "    f\"\"\"SELECT\n",
    "           event_name, event_params, user_properties, items\n",
    "        FROM `{BQ_PROJECT}.{BQ_DATASET}.{t}`\"\"\"\n",
    "    for t in latest_tbls[:1]\n",
    "])\n",
    "\n",
    "sql_more = f\"\"\"\n",
    "WITH base AS (\n",
    "  {union_sql}\n",
    "),\n",
    "sampled AS (\n",
    "  SELECT * FROM base\n",
    "  WHERE RAND() < {SAMPLE_RATE}\n",
    "  LIMIT {MAX_ROWS}\n",
    "),\n",
    "ev AS (\n",
    "  SELECT 'event_param' AS source, event_name,\n",
    "         ep.key AS param_key,\n",
    "         COALESCE(ep.value.string_value,\n",
    "                  CAST(ep.value.int_value AS STRING),\n",
    "                  CAST(ep.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(event_params) ep\n",
    "),\n",
    "up AS (\n",
    "  SELECT 'user_property' AS source, event_name,\n",
    "         up.key AS param_key,\n",
    "         COALESCE(up.value.string_value,\n",
    "                  CAST(up.value.int_value AS STRING),\n",
    "                  CAST(up.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(user_properties) up\n",
    "),\n",
    "ip AS (\n",
    "  SELECT 'item_param' AS source, event_name,\n",
    "         ip.key AS param_key,\n",
    "         COALESCE(ip.value.string_value,\n",
    "                  CAST(ip.value.int_value AS STRING),\n",
    "                  CAST(ip.value.double_value AS STRING)) AS param_value\n",
    "  FROM sampled, UNNEST(items) it, UNNEST(it.item_params) ip\n",
    "),\n",
    "flat AS (\n",
    "  SELECT * FROM ev\n",
    "  UNION ALL SELECT * FROM up\n",
    "  UNION ALL SELECT * FROM ip\n",
    "),\n",
    "flags AS (\n",
    "  SELECT\n",
    "    source, event_name, param_key, param_value,\n",
    "    REGEXP_CONTAINS(LOWER(param_key),\n",
    "      r'(email|e-mail|mail|name|firstname|first_name|lastname|last_name|phone|mobile|tel|address|street|zip|postcode|login|user|iban|ssn|nationalid|hetu)'\n",
    "    ) AS key_suspicious,\n",
    "    REGEXP_CONTAINS(LOWER(param_value),\n",
    "      r'^[a-z0-9._%+-]+@[a-z0-9.-]+\\\\.[a-z]{{2,}}$'\n",
    "    ) AS looks_email,\n",
    "    REGEXP_CONTAINS(param_value,\n",
    "      r'(?i)\\\\b(?:\\\\+?\\\\d{{1,3}}[\\\\s-]?)?(?:\\\\d[\\\\s-]?){8,}\\\\b'\n",
    "    ) AS looks_phone_digits,\n",
    "    REGEXP_CONTAINS(UPPER(param_value),\n",
    "      r'\\\\b[A-Z]{{2}}\\\\d{{2}}[A-Z0-9]{{11,30}}\\\\b'\n",
    "    ) AS looks_iban_generic,\n",
    "    REGEXP_CONTAINS(UPPER(param_value),\n",
    "      r'\\\\bFI\\\\d{{16}}\\\\b'\n",
    "    ) AS looks_iban_fi\n",
    "  FROM flat\n",
    ")\n",
    "SELECT\n",
    "  source, event_name, param_key,\n",
    "  COUNT(*) AS hits,\n",
    "  ARRAY_AGG(DISTINCT SUBSTR(param_value,1,120) IGNORE NULLS LIMIT 5) AS sample_values\n",
    "FROM flags\n",
    "WHERE key_suspicious OR looks_email OR looks_phone_digits OR looks_iban_generic OR looks_iban_fi\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY hits DESC, source, event_name, param_key\n",
    "\"\"\"\n",
    "\n",
    "dryrun_sql(sql_more)\n",
    "rows_more = run_sql(sql_more)\n",
    "print(\"Rows flagged (expanded):\", len(rows_more))\n",
    "rows_more[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d278b6-aa9b-4ab9-96c5-a4867b7e7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick whichever result list you want to export\n",
    "results = ext_rows if 'ext_rows' in locals() else (\n",
    "    rows_more if 'rows_more' in locals() else (\n",
    "        rows_min if 'rows_min' in locals() else pii_rows\n",
    "    )\n",
    ")\n",
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a69b5f-dcaa-408e-9522-02998c33e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = f\"{BQ_DATASET}_pii_audit_results.csv\"\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\n",
    "        \"source\",\"event_name\",\"param_key\",\"hits\",\n",
    "        \"by_key\",\"val_email\",\"val_phone\",\"val_ipv4\",\"val_iban\",\"val_credit_card\",\"val_hetu\",\n",
    "        \"sample_values\"\n",
    "    ])\n",
    "    for r in results:\n",
    "        w.writerow([\n",
    "            r.get(\"source\"),\n",
    "            r.get(\"event_name\"),\n",
    "            r.get(\"param_key\"),\n",
    "            r.get(\"hits\"),\n",
    "            r.get(\"by_key\"),\n",
    "            r.get(\"val_email\"),\n",
    "            r.get(\"val_phone\"),\n",
    "            r.get(\"val_ipv4\"),\n",
    "            r.get(\"val_iban\"),\n",
    "            r.get(\"val_credit_card\"),\n",
    "            r.get(\"val_hetu\"),\n",
    "            \"; \".join(r.get(\"sample_values\") or [])\n",
    "        ])\n",
    "\n",
    "print(f\"✅ Wrote: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f4016-5c8e-435c-9e4b-9a84a5e48b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Integrate anomaly detection (less sensitive)\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud.bigquery import QueryJobConfig\n",
    "\n",
    "# --- Tunables you can tweak ---\n",
    "LOOKBACK_DAYS   = 180        # longer baseline\n",
    "BYTES_CAP       = 2 * 1024**3\n",
    "TZ              = \"Europe/Helsinki\"  # for date bucketing in BQ\n",
    "# global guards\n",
    "MIN_COUNT_DAY   = 200        # require enough volume on the day\n",
    "REL_MIN_CHANGE  = 0.12       # require >= 12% relative deviation from baseline\n",
    "PERSIST_CONSEC  = 2          # require >= 2 consecutive days (set to 1 to disable)\n",
    "\n",
    "end_date   = datetime.utcnow().date()\n",
    "start_date = end_date - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "sql_anom = f\"\"\"\n",
    "SELECT\n",
    "  event_name,\n",
    "  DATE(TIMESTAMP_MICROS(event_timestamp), \"{TZ}\") AS event_date,\n",
    "  COUNT(1) AS event_count\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.events_*`\n",
    "WHERE\n",
    "  _TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL {LOOKBACK_DAYS} DAY))\n",
    "                   AND FORMAT_DATE('%Y%m%d', CURRENT_DATE())\n",
    "GROUP BY event_name, event_date\n",
    "ORDER BY event_name, event_date\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(sql_anom, job_config=QueryJobConfig(maximum_bytes_billed=BYTES_CAP)).to_dataframe()\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# Detectors (more conservative)\n",
    "# -----------------------------\n",
    "def robust_z_anoms(df, win=28, k=3.5, min_points=14, min_count=MIN_COUNT_DAY, rel_min=REL_MIN_CHANGE):\n",
    "    \"\"\"Median/MAD baseline with volume & effect-size guards.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\n",
    "    out = []\n",
    "    for ev, g in df.groupby(\"event_name\", sort=False):\n",
    "        g = g.sort_values(\"event_date\").copy()\n",
    "        if len(g) < min_points:\n",
    "            continue\n",
    "        # rolling robust baseline\n",
    "        g[\"median\"] = g[\"event_count\"].rolling(win, min_periods=win//2).median()\n",
    "        mad = g[\"event_count\"].rolling(win, min_periods=win//2).apply(\n",
    "            lambda x: np.median(np.abs(x - np.median(x))), raw=False\n",
    "        )\n",
    "        # sigma from MAD (avoid zero/NaN)\n",
    "        sigma = 1.4826 * mad.replace(0, np.nan)\n",
    "        g[\"robust_z\"] = (g[\"event_count\"] - g[\"median\"]) / sigma\n",
    "        g[\"rel_delta\"] = (g[\"event_count\"] - g[\"median\"]) / g[\"median\"].replace(0, np.nan)\n",
    "        g[\"anomaly_robustz_base\"] = (\n",
    "            g[\"event_count\"] >= min_count\n",
    "        ) & (\n",
    "            g[\"robust_z\"].abs() > k\n",
    "        ) & (\n",
    "            g[\"rel_delta\"].abs() >= rel_min\n",
    "        )\n",
    "        out.append(g[[\"event_name\",\"event_date\",\"event_count\",\"median\",\"robust_z\",\"rel_delta\",\"anomaly_robustz_base\"]])\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "def weekday_robust_anoms(df, lookback_weeks=12, k=3.0, min_points=21, min_count=MIN_COUNT_DAY, rel_min=REL_MIN_CHANGE):\n",
    "    \"\"\"Same-DOW robust baseline (median + MAD) over past N weeks.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\n",
    "    df[\"dow\"] = df[\"event_date\"].dt.dayofweek\n",
    "    out = []\n",
    "    for ev, g in df.groupby(\"event_name\", sort=False):\n",
    "        g = g.sort_values(\"event_date\").copy()\n",
    "        if len(g) < min_points:\n",
    "            continue\n",
    "        z_vals = []\n",
    "        rels = []\n",
    "        meds = []\n",
    "        for i in range(len(g)):\n",
    "            cur = g.iloc[i]\n",
    "            start = cur[\"event_date\"] - pd.Timedelta(days=7*lookback_weeks)\n",
    "            hist = g.loc[(g[\"dow\"] == cur[\"dow\"]) & (g[\"event_date\"] < cur[\"event_date\"]) & (g[\"event_date\"] >= start), \"event_count\"]\n",
    "            if len(hist) >= 6:\n",
    "                med = np.median(hist)\n",
    "                mad = np.median(np.abs(hist - med))\n",
    "                sigma = max(1e-6, 1.4826 * mad)\n",
    "                z = (cur[\"event_count\"] - med) / sigma\n",
    "                rel = (cur[\"event_count\"] - med) / (med if med else np.nan)\n",
    "            else:\n",
    "                med = np.nan; z = np.nan; rel = np.nan\n",
    "            meds.append(med); z_vals.append(z); rels.append(rel)\n",
    "        g[\"weekday_med\"] = meds\n",
    "        g[\"weekday_z\"] = z_vals\n",
    "        g[\"weekday_rel\"] = rels\n",
    "        g[\"anomaly_weekday_base\"] = (\n",
    "            (g[\"event_count\"] >= min_count)\n",
    "            & (g[\"weekday_z\"].abs() > k)\n",
    "            & (g[\"weekday_rel\"].abs() >= rel_min)\n",
    "        )\n",
    "        out.append(g[[\"event_name\",\"event_date\",\"event_count\",\"weekday_med\",\"weekday_z\",\"weekday_rel\",\"anomaly_weekday_base\"]])\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "def poisson_band_anoms(df, win=28, z=2.5, min_points=14, min_count=MIN_COUNT_DAY, rel_min=REL_MIN_CHANGE):\n",
    "    \"\"\"Poisson band with simple overdispersion from recent residuals.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\n",
    "    out = []\n",
    "    for ev, g in df.groupby(\"event_name\", sort=False):\n",
    "        g = g.sort_values(\"event_date\").copy()\n",
    "        if len(g) < min_points:\n",
    "            continue\n",
    "        lam = g[\"event_count\"].rolling(win, min_periods=win//2).mean()\n",
    "        # overdispersion phi from rolling median(|residual|)/0.6745\n",
    "        resid = g[\"event_count\"] - lam\n",
    "        sigma_rob = (resid.rolling(win, min_periods=win//2)\n",
    "                          .apply(lambda x: np.median(np.abs(x - np.median(x))) * 1.4826, raw=False))\n",
    "        sd = np.sqrt(np.maximum(lam, 1e-6))\n",
    "        # take larger of poisson sd and robust sd\n",
    "        sd_eff = np.maximum(sd, sigma_rob.fillna(sd))\n",
    "        g[\"lower\"] = (lam - z * sd_eff).clip(lower=0)\n",
    "        g[\"upper\"] = lam + z * sd_eff\n",
    "        g[\"rel_from_lam\"] = (g[\"event_count\"] - lam) / lam.replace(0, np.nan)\n",
    "        g[\"anomaly_poisson_base\"] = (\n",
    "            (g[\"event_count\"] >= min_count)\n",
    "            & ((g[\"event_count\"] < g[\"lower\"]) | (g[\"event_count\"] > g[\"upper\"]))\n",
    "            & (g[\"rel_from_lam\"].abs() >= rel_min)\n",
    "        )\n",
    "        out.append(g[[\"event_name\",\"event_date\",\"event_count\",\"lower\",\"upper\",\"rel_from_lam\",\"anomaly_poisson_base\"]])\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "def pct_change_anoms(df, win=14, up=0.35, down=0.35, min_points=14, min_base=MIN_COUNT_DAY):\n",
    "    \"\"\"Simple % change vs rolling mean with volume threshold.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\n",
    "    out = []\n",
    "    for ev, g in df.groupby(\"event_name\", sort=False):\n",
    "        g = g.sort_values(\"event_date\").copy()\n",
    "        if len(g) < min_points:\n",
    "            continue\n",
    "        base = g[\"event_count\"].rolling(win, min_periods=win//2).mean()\n",
    "        pct = (g[\"event_count\"] - base) / base.replace(0, np.nan)\n",
    "        g[\"pct_change\"] = pct.replace([np.inf, -np.inf], np.nan)\n",
    "        g[\"anomaly_pct_base\"] = (\n",
    "            (base >= min_base)\n",
    "            & ((g[\"pct_change\"] > up) | (g[\"pct_change\"] < -down))\n",
    "        )\n",
    "        out.append(g[[\"event_name\",\"event_date\",\"event_count\",\"pct_change\",\"anomaly_pct_base\"]])\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "JOIN_KEYS = [\"event_name\",\"event_date\",\"event_count\"]\n",
    "\n",
    "def _normalize_for_merge(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_in is None or df_in.empty:\n",
    "        return df_in\n",
    "    df = df_in.copy()\n",
    "    # ensure join key columns exist\n",
    "    for c in JOIN_KEYS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    # unify dtypes\n",
    "    df[\"event_name\"]  = df[\"event_name\"].astype(str)\n",
    "    df[\"event_date\"]  = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\\\n",
    "                          .dt.tz_localize(None).dt.normalize()  # midnight, no tz\n",
    "    df[\"event_count\"] = pd.to_numeric(df[\"event_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def safe_merge(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    if right is None or right.empty:\n",
    "        return left\n",
    "    L = _normalize_for_merge(left)\n",
    "    R = _normalize_for_merge(right)\n",
    "    return L.merge(R, on=JOIN_KEYS, how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run detectors\n",
    "# -----------------------------\n",
    "\n",
    "r1 = robust_z_anoms(df, win=28, k=3.5)\n",
    "r2 = weekday_robust_anoms(df, lookback_weeks=12, k=3.0)\n",
    "r3 = poisson_band_anoms(df, win=28, z=2.5)\n",
    "r4 = pct_change_anoms(df, win=14, up=0.35, down=0.35)\n",
    "\n",
    "m = _normalize_for_merge(df[JOIN_KEYS].copy())\n",
    "for x in [r1, r2, r3, r4]:\n",
    "    m = safe_merge(m, x)\n",
    "\n",
    "\n",
    "# fill NaNs\n",
    "for c in m.columns:\n",
    "    if c.startswith(\"anomaly_\") and c.endswith(\"_base\"):\n",
    "        m[c] = m[c].fillna(False)\n",
    "\n",
    "# optional persistence: require >= PERSIST_CONSEC consecutive anomaly days, same direction if available\n",
    "def apply_persistence(group, base_cols, min_run=PERSIST_CONSEC):\n",
    "    # choose a direction proxy (prefer robust_z, fallback to weekday_z or rel_from_lam)\n",
    "    dir_series = group.get(\"robust_z\")\n",
    "    if dir_series is None or dir_series.isna().all():          # ✅ FIXED: added the missing dot\n",
    "        dir_series = group.get(\"weekday_z\")\n",
    "    if dir_series is None or dir_series.isna().all():          # ✅ FIXED: added the missing dot here too\n",
    "        dir_series = group.get(\"rel_from_lam\")\n",
    "    direction = np.sign(dir_series).fillna(0)\n",
    "\n",
    "    base = group[base_cols].any(axis=1)\n",
    "\n",
    "    # compute run-lengths of consecutive True with same sign\n",
    "    run = np.zeros(len(base), dtype=int)\n",
    "    last_sign = 0\n",
    "    for i, (b, s) in enumerate(zip(base.values, direction.values)):\n",
    "        if i == 0 or not b or s == 0 or s != last_sign:\n",
    "            run[i] = 1 if b and s != 0 else 0\n",
    "        else:\n",
    "            run[i] = (run[i-1] + 1) if b else 0\n",
    "        last_sign = s if b and s != 0 else 0\n",
    "    persistent = run >= min_run if min_run > 1 else base.values\n",
    "    group = group.copy()\n",
    "    group[\"anomaly_any_base\"] = base.values\n",
    "    group[\"anomaly_any\"] = persistent\n",
    "    return group\n",
    "\n",
    "\n",
    "base_cols = [c for c in m.columns if c.startswith(\"anomaly_\") and c.endswith(\"_base\")]\n",
    "m = m.groupby(\"event_name\", group_keys=False).apply(lambda g: apply_persistence(g.sort_values(\"event_date\"), base_cols, PERSIST_CONSEC))\n",
    "\n",
    "# cast booleans\n",
    "flag_cols = [\"anomaly_any_base\",\"anomaly_any\"] + base_cols\n",
    "for c in flag_cols:\n",
    "    m[c] = m[c].astype(\"boolean\")\n",
    "\n",
    "df_all_flags = m[m[\"anomaly_any\"]].sort_values([\"event_name\",\"event_date\"])\n",
    "\n",
    "print(\"🚨 total anomalies (with persistence):\", int(df_all_flags.shape[0]))\n",
    "display_cols = [\"event_name\",\"event_date\",\"event_count\",\"robust_z\",\"rel_delta\",\"weekday_z\",\"rel_from_lam\",\"pct_change\",\"anomaly_any\"]\n",
    "print(df_all_flags[display_cols].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838ced6-40b4-4aaf-83ef-2ac9b50ccc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tracking-spec)",
   "language": "python",
   "name": "tracking-spec-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
